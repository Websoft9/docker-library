# Open WebUI

## Guide

### Quick Start

1. Create administrator credential when Open WebUI setup
2. Go to the **Settings > Admin Settings** to add a LLM, e.g [tinydolphin](https://ollama.com/library/tinydolphin) which not exceeding 1GB
3. You can select **tinydolphin** for chat now

### Running with GPU?

1. Select version **cuda** when create this application
2. Edit this application with Compose and change `docker-compose-gpu.yml` to `docker-compose.yml`
3. Recreate this application

### Config

- Config Ollama URL: **设置 > 管理员设置**
- Mutiply Languages: Yes

## FAQ
